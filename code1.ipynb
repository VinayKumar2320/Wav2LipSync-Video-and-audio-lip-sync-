{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Wav2Lip:\n\tMissing key(s) in state_dict: \"face_encoder_blocks.0.0.conv_block.0.weight\", \"face_encoder_blocks.0.0.conv_block.0.bias\", \"face_encoder_blocks.0.0.conv_block.1.weight\", \"face_encoder_blocks.0.0.conv_block.1.bias\", \"face_encoder_blocks.0.0.conv_block.1.running_mean\", \"face_encoder_blocks.0.0.conv_block.1.running_var\", \"face_encoder_blocks.1.0.conv_block.0.weight\", \"face_encoder_blocks.1.0.conv_block.0.bias\", \"face_encoder_blocks.1.0.conv_block.1.weight\", \"face_encoder_blocks.1.0.conv_block.1.bias\", \"face_encoder_blocks.1.0.conv_block.1.running_mean\", \"face_encoder_blocks.1.0.conv_block.1.running_var\", \"face_encoder_blocks.1.1.conv_block.0.weight\", \"face_encoder_blocks.1.1.conv_block.0.bias\", \"face_encoder_blocks.1.1.conv_block.1.weight\", \"face_encoder_blocks.1.1.conv_block.1.bias\", \"face_encoder_blocks.1.1.conv_block.1.running_mean\", \"face_encoder_blocks.1.1.conv_block.1.running_var\", \"face_encoder_blocks.1.2.conv_block.0.weight\", \"face_encoder_blocks.1.2.conv_block.0.bias\", \"face_encoder_blocks.1.2.conv_block.1.weight\", \"face_encoder_blocks.1.2.conv_block.1.bias\", \"face_encoder_blocks.1.2.conv_block.1.running_mean\", \"face_encoder_blocks.1.2.conv_block.1.running_var\", \"face_encoder_blocks.2.0.conv_block.0.weight\", \"face_encoder_blocks.2.0.conv_block.0.bias\", \"face_encoder_blocks.2.0.conv_block.1.weight\", \"face_encoder_blocks.2.0.conv_block.1.bias\", \"face_encoder_blocks.2.0.conv_block.1.running_mean\", \"face_encoder_blocks.2.0.conv_block.1.running_var\", \"face_encoder_blocks.2.1.conv_block.0.weight\", \"face_encoder_blocks.2.1.conv_block.0.bias\", \"face_encoder_blocks.2.1.conv_block.1.weight\", \"face_encoder_blocks.2.1.conv_block.1.bias\", \"face_encoder_blocks.2.1.conv_block.1.running_mean\", \"face_encoder_blocks.2.1.conv_block.1.running_var\", \"face_encoder_blocks.2.2.conv_block.0.weight\", \"face_encoder_blocks.2.2.conv_block.0.bias\", \"face_encoder_blocks.2.2.conv_block.1.weight\", \"face_encoder_blocks.2.2.conv_block.1.bias\", \"face_encoder_blocks.2.2.conv_block.1.running_mean\", \"face_encoder_blocks.2.2.conv_block.1.running_var\", \"face_encoder_blocks.2.3.conv_block.0.weight\", \"face_encoder_blocks.2.3.conv_block.0.bias\", \"face_encoder_blocks.2.3.conv_block.1.weight\", \"face_encoder_blocks.2.3.conv_block.1.bias\", \"face_encoder_blocks.2.3.conv_block.1.running_mean\", \"face_encoder_blocks.2.3.conv_block.1.running_var\", \"face_encoder_blocks.3.0.conv_block.0.weight\", \"face_encoder_blocks.3.0.conv_block.0.bias\", \"face_encoder_blocks.3.0.conv_block.1.weight\", \"face_encoder_blocks.3.0.conv_block.1.bias\", \"face_encoder_blocks.3.0.conv_block.1.running_mean\", \"face_encoder_blocks.3.0.conv_block.1.running_var\", \"face_encoder_blocks.3.1.conv_block.0.weight\", \"face_encoder_blocks.3.1.conv_block.0.bias\", \"face_encoder_blocks.3.1.conv_block.1.weight\", \"face_encoder_blocks.3.1.conv_block.1.bias\", \"face_encoder_blocks.3.1.conv_block.1.running_mean\", \"face_encoder_blocks.3.1.conv_block.1.running_var\", \"face_encoder_blocks.3.2.conv_block.0.weight\", \"face_encoder_blocks.3.2.conv_block.0.bias\", \"face_encoder_blocks.3.2.conv_block.1.weight\", \"face_encoder_blocks.3.2.conv_block.1.bias\", \"face_encoder_blocks.3.2.conv_block.1.running_mean\", \"face_encoder_blocks.3.2.conv_block.1.running_var\", \"face_encoder_blocks.4.0.conv_block.0.weight\", \"face_encoder_blocks.4.0.conv_block.0.bias\", \"face_encoder_blocks.4.0.conv_block.1.weight\", \"face_encoder_blocks.4.0.conv_block.1.bias\", \"face_encoder_blocks.4.0.conv_block.1.running_mean\", \"face_encoder_blocks.4.0.conv_block.1.running_var\", \"face_encoder_blocks.4.1.conv_block.0.weight\", \"face_encoder_blocks.4.1.conv_block.0.bias\", \"face_encoder_blocks.4.1.conv_block.1.weight\", \"face_encoder_blocks.4.1.conv_block.1.bias\", \"face_encoder_blocks.4.1.conv_block.1.running_mean\", \"face_encoder_blocks.4.1.conv_block.1.running_var\", \"face_encoder_blocks.4.2.conv_block.0.weight\", \"face_encoder_blocks.4.2.conv_block.0.bias\", \"face_encoder_blocks.4.2.conv_block.1.weight\", \"face_encoder_blocks.4.2.conv_block.1.bias\", \"face_encoder_blocks.4.2.conv_block.1.running_mean\", \"face_encoder_blocks.4.2.conv_block.1.running_var\", \"face_encoder_blocks.5.0.conv_block.0.weight\", \"face_encoder_blocks.5.0.conv_block.0.bias\", \"face_encoder_blocks.5.0.conv_block.1.weight\", \"face_encoder_blocks.5.0.conv_block.1.bias\", \"face_encoder_blocks.5.0.conv_block.1.running_mean\", \"face_encoder_blocks.5.0.conv_block.1.running_var\", \"face_encoder_blocks.5.1.conv_block.0.weight\", \"face_encoder_blocks.5.1.conv_block.0.bias\", \"face_encoder_blocks.5.1.conv_block.1.weight\", \"face_encoder_blocks.5.1.conv_block.1.bias\", \"face_encoder_blocks.5.1.conv_block.1.running_mean\", \"face_encoder_blocks.5.1.conv_block.1.running_var\", \"face_encoder_blocks.6.0.conv_block.0.weight\", \"face_encoder_blocks.6.0.conv_block.0.bias\", \"face_encoder_blocks.6.0.conv_block.1.weight\", \"face_encoder_blocks.6.0.conv_block.1.bias\", \"face_encoder_blocks.6.0.conv_block.1.running_mean\", \"face_encoder_blocks.6.0.conv_block.1.running_var\", \"face_encoder_blocks.6.1.conv_block.0.weight\", \"face_encoder_blocks.6.1.conv_block.0.bias\", \"face_encoder_blocks.6.1.conv_block.1.weight\", \"face_encoder_blocks.6.1.conv_block.1.bias\", \"face_encoder_blocks.6.1.conv_block.1.running_mean\", \"face_encoder_blocks.6.1.conv_block.1.running_var\", \"audio_encoder.0.conv_block.0.weight\", \"audio_encoder.0.conv_block.0.bias\", \"audio_encoder.0.conv_block.1.weight\", \"audio_encoder.0.conv_block.1.bias\", \"audio_encoder.0.conv_block.1.running_mean\", \"audio_encoder.0.conv_block.1.running_var\", \"audio_encoder.1.conv_block.0.weight\", \"audio_encoder.1.conv_block.0.bias\", \"audio_encoder.1.conv_block.1.weight\", \"audio_encoder.1.conv_block.1.bias\", \"audio_encoder.1.conv_block.1.running_mean\", \"audio_encoder.1.conv_block.1.running_var\", \"audio_encoder.2.conv_block.0.weight\", \"audio_encoder.2.conv_block.0.bias\", \"audio_encoder.2.conv_block.1.weight\", \"audio_encoder.2.conv_block.1.bias\", \"audio_encoder.2.conv_block.1.running_mean\", \"audio_encoder.2.conv_block.1.running_var\", \"audio_encoder.3.conv_block.0.weight\", \"audio_encoder.3.conv_block.0.bias\", \"audio_encoder.3.conv_block.1.weight\", \"audio_encoder.3.conv_block.1.bias\", \"audio_encoder.3.conv_block.1.running_mean\", \"audio_encoder.3.conv_block.1.running_var\", \"audio_encoder.4.conv_block.0.weight\", \"audio_encoder.4.conv_block.0.bias\", \"audio_encoder.4.conv_block.1.weight\", \"audio_encoder.4.conv_block.1.bias\", \"audio_encoder.4.conv_block.1.running_mean\", \"audio_encoder.4.conv_block.1.running_var\", \"audio_encoder.5.conv_block.0.weight\", \"audio_encoder.5.conv_block.0.bias\", \"audio_encoder.5.conv_block.1.weight\", \"audio_encoder.5.conv_block.1.bias\", \"audio_encoder.5.conv_block.1.running_mean\", \"audio_encoder.5.conv_block.1.running_var\", \"audio_encoder.6.conv_block.0.weight\", \"audio_encoder.6.conv_block.0.bias\", \"audio_encoder.6.conv_block.1.weight\", \"audio_encoder.6.conv_block.1.bias\", \"audio_encoder.6.conv_block.1.running_mean\", \"audio_encoder.6.conv_block.1.running_var\", \"audio_encoder.7.conv_block.0.weight\", \"audio_encoder.7.conv_block.0.bias\", \"audio_encoder.7.conv_block.1.weight\", \"audio_encoder.7.conv_block.1.bias\", \"audio_encoder.7.conv_block.1.running_mean\", \"audio_encoder.7.conv_block.1.running_var\", \"audio_encoder.8.conv_block.0.weight\", \"audio_encoder.8.conv_block.0.bias\", \"audio_encoder.8.conv_block.1.weight\", \"audio_encoder.8.conv_block.1.bias\", \"audio_encoder.8.conv_block.1.running_mean\", \"audio_encoder.8.conv_block.1.running_var\", \"audio_encoder.9.conv_block.0.weight\", \"audio_encoder.9.conv_block.0.bias\", \"audio_encoder.9.conv_block.1.weight\", \"audio_encoder.9.conv_block.1.bias\", \"audio_encoder.9.conv_block.1.running_mean\", \"audio_encoder.9.conv_block.1.running_var\", \"audio_encoder.10.conv_block.0.weight\", \"audio_encoder.10.conv_block.0.bias\", \"audio_encoder.10.conv_block.1.weight\", \"audio_encoder.10.conv_block.1.bias\", \"audio_encoder.10.conv_block.1.running_mean\", \"audio_encoder.10.conv_block.1.running_var\", \"audio_encoder.11.conv_block.0.weight\", \"audio_encoder.11.conv_block.0.bias\", \"audio_encoder.11.conv_block.1.weight\", \"audio_encoder.11.conv_block.1.bias\", \"audio_encoder.11.conv_block.1.running_mean\", \"audio_encoder.11.conv_block.1.running_var\", \"audio_encoder.12.conv_block.0.weight\", \"audio_encoder.12.conv_block.0.bias\", \"audio_encoder.12.conv_block.1.weight\", \"audio_encoder.12.conv_block.1.bias\", \"audio_encoder.12.conv_block.1.running_mean\", \"audio_encoder.12.conv_block.1.running_var\", \"face_decoder_blocks.0.0.conv_block.0.weight\", \"face_decoder_blocks.0.0.conv_block.0.bias\", \"face_decoder_blocks.0.0.conv_block.1.weight\", \"face_decoder_blocks.0.0.conv_block.1.bias\", \"face_decoder_blocks.0.0.conv_block.1.running_mean\", \"face_decoder_blocks.0.0.conv_block.1.running_var\", \"face_decoder_blocks.1.0.conv_block.0.weight\", \"face_decoder_blocks.1.0.conv_block.0.bias\", \"face_decoder_blocks.1.0.conv_block.1.weight\", \"face_decoder_blocks.1.0.conv_block.1.bias\", \"face_decoder_blocks.1.0.conv_block.1.running_mean\", \"face_decoder_blocks.1.0.conv_block.1.running_var\", \"face_decoder_blocks.1.1.conv_block.0.weight\", \"face_decoder_blocks.1.1.conv_block.0.bias\", \"face_decoder_blocks.1.1.conv_block.1.weight\", \"face_decoder_blocks.1.1.conv_block.1.bias\", \"face_decoder_blocks.1.1.conv_block.1.running_mean\", \"face_decoder_blocks.1.1.conv_block.1.running_var\", \"face_decoder_blocks.2.0.conv_block.0.weight\", \"face_decoder_blocks.2.0.conv_block.0.bias\", \"face_decoder_blocks.2.0.conv_block.1.weight\", \"face_decoder_blocks.2.0.conv_block.1.bias\", \"face_decoder_blocks.2.0.conv_block.1.running_mean\", \"face_decoder_blocks.2.0.conv_block.1.running_var\", \"face_decoder_blocks.2.1.conv_block.0.weight\", \"face_decoder_blocks.2.1.conv_block.0.bias\", \"face_decoder_blocks.2.1.conv_block.1.weight\", \"face_decoder_blocks.2.1.conv_block.1.bias\", \"face_decoder_blocks.2.1.conv_block.1.running_mean\", \"face_decoder_blocks.2.1.conv_block.1.running_var\", \"face_decoder_blocks.2.2.conv_block.0.weight\", \"face_decoder_blocks.2.2.conv_block.0.bias\", \"face_decoder_blocks.2.2.conv_block.1.weight\", \"face_decoder_blocks.2.2.conv_block.1.bias\", \"face_decoder_blocks.2.2.conv_block.1.running_mean\", \"face_decoder_blocks.2.2.conv_block.1.running_var\", \"face_decoder_blocks.3.0.conv_block.0.weight\", \"face_decoder_blocks.3.0.conv_block.0.bias\", \"face_decoder_blocks.3.0.conv_block.1.weight\", \"face_decoder_blocks.3.0.conv_block.1.bias\", \"face_decoder_blocks.3.0.conv_block.1.running_mean\", \"face_decoder_blocks.3.0.conv_block.1.running_var\", \"face_decoder_blocks.3.1.conv_block.0.weight\", \"face_decoder_blocks.3.1.conv_block.0.bias\", \"face_decoder_blocks.3.1.conv_block.1.weight\", \"face_decoder_blocks.3.1.conv_block.1.bias\", \"face_decoder_blocks.3.1.conv_block.1.running_mean\", \"face_decoder_blocks.3.1.conv_block.1.running_var\", \"face_decoder_blocks.3.2.conv_block.0.weight\", \"face_decoder_blocks.3.2.conv_block.0.bias\", \"face_decoder_blocks.3.2.conv_block.1.weight\", \"face_decoder_blocks.3.2.conv_block.1.bias\", \"face_decoder_blocks.3.2.conv_block.1.running_mean\", \"face_decoder_blocks.3.2.conv_block.1.running_var\", \"face_decoder_blocks.4.0.conv_block.0.weight\", \"face_decoder_blocks.4.0.conv_block.0.bias\", \"face_decoder_blocks.4.0.conv_block.1.weight\", \"face_decoder_blocks.4.0.conv_block.1.bias\", \"face_decoder_blocks.4.0.conv_block.1.running_mean\", \"face_decoder_blocks.4.0.conv_block.1.running_var\", \"face_decoder_blocks.4.1.conv_block.0.weight\", \"face_decoder_blocks.4.1.conv_block.0.bias\", \"face_decoder_blocks.4.1.conv_block.1.weight\", \"face_decoder_blocks.4.1.conv_block.1.bias\", \"face_decoder_blocks.4.1.conv_block.1.running_mean\", \"face_decoder_blocks.4.1.conv_block.1.running_var\", \"face_decoder_blocks.4.2.conv_block.0.weight\", \"face_decoder_blocks.4.2.conv_block.0.bias\", \"face_decoder_blocks.4.2.conv_block.1.weight\", \"face_decoder_blocks.4.2.conv_block.1.bias\", \"face_decoder_blocks.4.2.conv_block.1.running_mean\", \"face_decoder_blocks.4.2.conv_block.1.running_var\", \"face_decoder_blocks.5.0.conv_block.0.weight\", \"face_decoder_blocks.5.0.conv_block.0.bias\", \"face_decoder_blocks.5.0.conv_block.1.weight\", \"face_decoder_blocks.5.0.conv_block.1.bias\", \"face_decoder_blocks.5.0.conv_block.1.running_mean\", \"face_decoder_blocks.5.0.conv_block.1.running_var\", \"face_decoder_blocks.5.1.conv_block.0.weight\", \"face_decoder_blocks.5.1.conv_block.0.bias\", \"face_decoder_blocks.5.1.conv_block.1.weight\", \"face_decoder_blocks.5.1.conv_block.1.bias\", \"face_decoder_blocks.5.1.conv_block.1.running_mean\", \"face_decoder_blocks.5.1.conv_block.1.running_var\", \"face_decoder_blocks.5.2.conv_block.0.weight\", \"face_decoder_blocks.5.2.conv_block.0.bias\", \"face_decoder_blocks.5.2.conv_block.1.weight\", \"face_decoder_blocks.5.2.conv_block.1.bias\", \"face_decoder_blocks.5.2.conv_block.1.running_mean\", \"face_decoder_blocks.5.2.conv_block.1.running_var\", \"face_decoder_blocks.6.0.conv_block.0.weight\", \"face_decoder_blocks.6.0.conv_block.0.bias\", \"face_decoder_blocks.6.0.conv_block.1.weight\", \"face_decoder_blocks.6.0.conv_block.1.bias\", \"face_decoder_blocks.6.0.conv_block.1.running_mean\", \"face_decoder_blocks.6.0.conv_block.1.running_var\", \"face_decoder_blocks.6.1.conv_block.0.weight\", \"face_decoder_blocks.6.1.conv_block.0.bias\", \"face_decoder_blocks.6.1.conv_block.1.weight\", \"face_decoder_blocks.6.1.conv_block.1.bias\", \"face_decoder_blocks.6.1.conv_block.1.running_mean\", \"face_decoder_blocks.6.1.conv_block.1.running_var\", \"face_decoder_blocks.6.2.conv_block.0.weight\", \"face_decoder_blocks.6.2.conv_block.0.bias\", \"face_decoder_blocks.6.2.conv_block.1.weight\", \"face_decoder_blocks.6.2.conv_block.1.bias\", \"face_decoder_blocks.6.2.conv_block.1.running_mean\", \"face_decoder_blocks.6.2.conv_block.1.running_var\", \"output_block.0.conv_block.0.weight\", \"output_block.0.conv_block.0.bias\", \"output_block.0.conv_block.1.weight\", \"output_block.0.conv_block.1.bias\", \"output_block.0.conv_block.1.running_mean\", \"output_block.0.conv_block.1.running_var\", \"output_block.1.weight\", \"output_block.1.bias\". \n\tUnexpected key(s) in state_dict: \"state_dict\", \"optimizer\", \"global_step\", \"global_epoch\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/vinaykumar/Documents/Documents/Vinay Data/Openin ass/code1.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vinaykumar/Documents/Documents/Vinay%20Data/Openin%20ass/code1.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m checkpoint_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m//Users/vinaykumar/Documents/Documents/Vinay Data/Openin ass/checkpoints/wav2lip.pth\u001b[39m\u001b[39m'\u001b[39m  \u001b[39m# Replace with the correct path to the model checkpoint\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vinaykumar/Documents/Documents/Vinay%20Data/Openin%20ass/code1.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m model_state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(checkpoint_path, map_location\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/vinaykumar/Documents/Documents/Vinay%20Data/Openin%20ass/code1.ipynb#W0sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m model\u001b[39m.\u001b[39;49mload_state_dict(model_state_dict)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vinaykumar/Documents/Documents/Vinay%20Data/Openin%20ass/code1.ipynb#W0sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vinaykumar/Documents/Documents/Vinay%20Data/Openin%20ass/code1.ipynb#W0sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Wav2Lip:\n\tMissing key(s) in state_dict: \"face_encoder_blocks.0.0.conv_block.0.weight\", \"face_encoder_blocks.0.0.conv_block.0.bias\", \"face_encoder_blocks.0.0.conv_block.1.weight\", \"face_encoder_blocks.0.0.conv_block.1.bias\", \"face_encoder_blocks.0.0.conv_block.1.running_mean\", \"face_encoder_blocks.0.0.conv_block.1.running_var\", \"face_encoder_blocks.1.0.conv_block.0.weight\", \"face_encoder_blocks.1.0.conv_block.0.bias\", \"face_encoder_blocks.1.0.conv_block.1.weight\", \"face_encoder_blocks.1.0.conv_block.1.bias\", \"face_encoder_blocks.1.0.conv_block.1.running_mean\", \"face_encoder_blocks.1.0.conv_block.1.running_var\", \"face_encoder_blocks.1.1.conv_block.0.weight\", \"face_encoder_blocks.1.1.conv_block.0.bias\", \"face_encoder_blocks.1.1.conv_block.1.weight\", \"face_encoder_blocks.1.1.conv_block.1.bias\", \"face_encoder_blocks.1.1.conv_block.1.running_mean\", \"face_encoder_blocks.1.1.conv_block.1.running_var\", \"face_encoder_blocks.1.2.conv_block.0.weight\", \"face_encoder_blocks.1.2.conv_block.0.bias\", \"face_encoder_blocks.1.2.conv_block.1.weight\", \"face_encoder_blocks.1.2.conv_block.1.bias\", \"face_encoder_blocks.1.2.conv_block.1.running_mean\", \"face_encoder_blocks.1.2.conv_block.1.running_var\", \"face_encoder_blocks.2.0.conv_block.0.weight\", \"face_encoder_blocks.2.0.conv_block.0.bias\", \"face_encoder_blocks.2.0.conv_block.1.weight\", \"face_encoder_blocks.2.0.conv_block.1.bias\", \"face_encoder_blocks.2.0.conv_block.1.running_mean\", \"face_encoder_blocks.2.0.conv_block.1.running_var\", \"face_encoder_blocks.2.1.conv_block.0.weight\", \"face_encoder_blocks.2.1.conv_block.0.bias\", \"face_encoder_blocks.2.1.conv_block.1.weight\", \"face_encoder_blocks.2.1.conv_block.1.bias\", \"face_encoder_blocks.2.1.conv_block.1.running_mean\", \"face_encoder_blocks.2.1.conv_block.1.running_var\", \"face_encoder_blocks.2.2.conv_block.0.weight\", \"face_encoder_blocks.2.2.conv_block.0.bias\", \"face_encoder_blocks.2.2.conv_block.1.weight\", \"face_encoder_blocks.2.2.conv_block.1.bias\", \"face_encoder_blocks.2.2.conv_block.1.running_mean\", \"face_encoder_blocks.2.2.conv_block.1.running_var\", \"face_encoder_blocks.2.3.conv_block.0.weight\", \"face_encoder_blocks.2.3.conv_block.0.bias\", \"face_encoder_blocks.2.3.conv_block.1.weight\", \"face_encoder_blocks.2.3.conv_block.1.bias\", \"face_encoder_blocks.2.3.conv_block.1.running_mean\", \"face_encoder_blocks.2.3.conv_block.1.running_var\", \"face_encoder_blocks.3.0.conv_block.0.weight\", \"face_encoder_blocks.3.0.conv_block.0.bias\", \"face_encoder_blocks.3.0.conv_block.1.weight\", \"face_encoder_blocks.3.0.conv_block.1.bias\", \"face_encoder_blocks.3.0.conv_block.1.running_mean\", \"face_encoder_blocks.3.0.conv_block.1.running_var\", \"face_encoder_blocks.3.1.conv_block.0.weight\", \"face_encoder_blocks.3.1.conv_block.0.bias\", \"face_encoder_blocks.3.1.conv_block.1.weight\", \"face_encoder_blocks.3.1.conv_block.1.bias\", \"face_encoder_blocks.3.1.conv_block.1.running_mean\", \"face_encoder_blocks.3.1.conv_block.1.running_var\", \"face_encoder_blocks.3.2.conv_block.0.weight\", \"face_encoder_blocks.3.2.conv_block.0.bias\", \"face_encoder_blocks.3.2.conv_block.1.weight\", \"face_encoder_blocks.3.2.conv_block.1.bias\", \"face_encoder_blocks.3.2.conv_block.1.running_mean\", \"face_encoder_blocks.3.2.conv_block.1.running_var\", \"face_encoder_blocks.4.0.conv_block.0.weight\", \"face_encoder_blocks.4.0.conv_block.0.bias\", \"face_encoder_blocks.4.0.conv_block.1.weight\", \"face_encoder_blocks.4.0.conv_block.1.bias\", \"face_encoder_blocks.4.0.conv_block.1.running_mean\", \"face_encoder_blocks.4.0.conv_block.1.running_var\", \"face_encoder_blocks.4.1.conv_block.0.weight\", \"face_encoder_blocks.4.1.conv_block.0.bias\", \"face_encoder_blocks.4.1.conv_block.1.weight\", \"face_encoder_blocks.4.1.conv_block.1.bias\", \"face_encoder_blocks.4.1.conv_block.1.running_mean\", \"face_encoder_blocks.4.1.conv_block.1.running_var\", \"face_encoder_blocks.4.2.conv_block.0.weight\", \"face_encoder_blocks.4.2.conv_block.0.bias\", \"face_encoder_blocks.4.2.conv_block.1.weight\", \"face_encoder_blocks.4.2.conv_block.1.bias\", \"face_encoder_blocks.4.2.conv_block.1.running_mean\", \"face_encoder_blocks.4.2.conv_block.1.running_var\", \"face_encoder_blocks.5.0.conv_block.0.weight\", \"face_encoder_blocks.5.0.conv_block.0.bias\", \"face_encoder_blocks.5.0.conv_block.1.weight\", \"face_encoder_blocks.5.0.conv_block.1.bias\", \"face_encoder_blocks.5.0.conv_block.1.running_mean\", \"face_encoder_blocks.5.0.conv_block.1.running_var\", \"face_encoder_blocks.5.1.conv_block.0.weight\", \"face_encoder_blocks.5.1.conv_block.0.bias\", \"face_encoder_blocks.5.1.conv_block.1.weight\", \"face_encoder_blocks.5.1.conv_block.1.bias\", \"face_encoder_blocks.5.1.conv_block.1.running_mean\", \"face_encoder_blocks.5.1.conv_block.1.running_var\", \"face_encoder_blocks.6.0.conv_block.0.weight\", \"face_encoder_blocks.6.0.conv_block.0.bias\", \"face_encoder_blocks.6.0.conv_block.1.weight\", \"face_encoder_blocks.6.0.conv_block.1.bias\", \"face_encoder_blocks.6.0.conv_block.1.running_mean\", \"face_encoder_blocks.6.0.conv_block.1.running_var\", \"face_encoder_blocks.6.1.conv_block.0.weight\", \"face_encoder_blocks.6.1.conv_block.0.bias\", \"face_encoder_blocks.6.1.conv_block.1.weight\", \"face_encoder_blocks.6.1.conv_block.1.bias\", \"face_encoder_blocks.6.1.conv_block.1.running_mean\", \"face_encoder_blocks.6.1.conv_block.1.running_var\", \"audio_encoder.0.conv_block.0.weight\", \"audio_encoder.0.conv_block.0.bias\", \"audio_encoder.0.conv_block.1.weight\", \"audio_encoder.0.conv_block.1.bias\", \"audio_encoder.0.conv_block.1.running_mean\", \"audio_encoder.0.conv_block.1.running_var\", \"audio_encoder.1.conv_block.0.weight\", \"audio_encoder.1.conv_block.0.bias\", \"audio_encoder.1.conv_block.1.weight\", \"audio_encoder.1.conv_block.1.bias\", \"audio_encoder.1.conv_block.1.running_mean\", \"audio_encoder.1.conv_block.1.running_var\", \"audio_encoder.2.conv_block.0.weight\", \"audio_encoder.2.conv_block.0.bias\", \"audio_encoder.2.conv_block.1.weight\", \"audio_encoder.2.conv_block.1.bias\", \"audio_encoder.2.conv_block.1.running_mean\", \"audio_encoder.2.conv_block.1.running_var\", \"audio_encoder.3.conv_block.0.weight\", \"audio_encoder.3.conv_block.0.bias\", \"audio_encoder.3.conv_block.1.weight\", \"audio_encoder.3.conv_block.1.bias\", \"audio_encoder.3.conv_block.1.running_mean\", \"audio_encoder.3.conv_block.1.running_var\", \"audio_encoder.4.conv_block.0.weight\", \"audio_encoder.4.conv_block.0.bias\", \"audio_encoder.4.conv_block.1.weight\", \"audio_encoder.4.conv_block.1.bias\", \"audio_encoder.4.conv_block.1.running_mean\", \"audio_encoder.4.conv_block.1.running_var\", \"audio_encoder.5.conv_block.0.weight\", \"audio_encoder.5.conv_block.0.bias\", \"audio_encoder.5.conv_block.1.weight\", \"audio_encoder.5.conv_block.1.bias\", \"audio_encoder.5.conv_block.1.running_mean\", \"audio_encoder.5.conv_block.1.running_var\", \"audio_encoder.6.conv_block.0.weight\", \"audio_encoder.6.conv_block.0.bias\", \"audio_encoder.6.conv_block.1.weight\", \"audio_encoder.6.conv_block.1.bias\", \"audio_encoder.6.conv_block.1.running_mean\", \"audio_encoder.6.conv_block.1.running_var\", \"audio_encoder.7.conv_block.0.weight\", \"audio_encoder.7.conv_block.0.bias\", \"audio_encoder.7.conv_block.1.weight\", \"audio_encoder.7.conv_block.1.bias\", \"audio_encoder.7.conv_block.1.running_mean\", \"audio_encoder.7.conv_block.1.running_var\", \"audio_encoder.8.conv_block.0.weight\", \"audio_encoder.8.conv_block.0.bias\", \"audio_encoder.8.conv_block.1.weight\", \"audio_encoder.8.conv_block.1.bias\", \"audio_encoder.8.conv_block.1.running_mean\", \"audio_encoder.8.conv_block.1.running_var\", \"audio_encoder.9.conv_block.0.weight\", \"audio_encoder.9.conv_block.0.bias\", \"audio_encoder.9.conv_block.1.weight\", \"audio_encoder.9.conv_block.1.bias\", \"audio_encoder.9.conv_block.1.running_mean\", \"audio_encoder.9.conv_block.1.running_var\", \"audio_encoder.10.conv_block.0.weight\", \"audio_encoder.10.conv_block.0.bias\", \"audio_encoder.10.conv_block.1.weight\", \"audio_encoder.10.conv_block.1.bias\", \"audio_encoder.10.conv_block.1.running_mean\", \"audio_encoder.10.conv_block.1.running_var\", \"audio_encoder.11.conv_block.0.weight\", \"audio_encoder.11.conv_block.0.bias\", \"audio_encoder.11.conv_block.1.weight\", \"audio_encoder.11.conv_block.1.bias\", \"audio_encoder.11.conv_block.1.running_mean\", \"audio_encoder.11.conv_block.1.running_var\", \"audio_encoder.12.conv_block.0.weight\", \"audio_encoder.12.conv_block.0.bias\", \"audio_encoder.12.conv_block.1.weight\", \"audio_encoder.12.conv_block.1.bias\", \"audio_encoder.12.conv_block.1.running_mean\", \"audio_encoder.12.conv_block.1.running_var\", \"face_decoder_blocks.0.0.conv_block.0.weight\", \"face_decoder_blocks.0.0.conv_block.0.bias\", \"face_decoder_blocks.0.0.conv_block.1.weight\", \"face_decoder_blocks.0.0.conv_block.1.bias\", \"face_decoder_blocks.0.0.conv_block.1.running_mean\", \"face_decoder_blocks.0.0.conv_block.1.running_var\", \"face_decoder_blocks.1.0.conv_block.0.weight\", \"face_decoder_blocks.1.0.conv_block.0.bias\", \"face_decoder_blocks.1.0.conv_block.1.weight\", \"face_decoder_blocks.1.0.conv_block.1.bias\", \"face_decoder_blocks.1.0.conv_block.1.running_mean\", \"face_decoder_blocks.1.0.conv_block.1.running_var\", \"face_decoder_blocks.1.1.conv_block.0.weight\", \"face_decoder_blocks.1.1.conv_block.0.bias\", \"face_decoder_blocks.1.1.conv_block.1.weight\", \"face_decoder_blocks.1.1.conv_block.1.bias\", \"face_decoder_blocks.1.1.conv_block.1.running_mean\", \"face_decoder_blocks.1.1.conv_block.1.running_var\", \"face_decoder_blocks.2.0.conv_block.0.weight\", \"face_decoder_blocks.2.0.conv_block.0.bias\", \"face_decoder_blocks.2.0.conv_block.1.weight\", \"face_decoder_blocks.2.0.conv_block.1.bias\", \"face_decoder_blocks.2.0.conv_block.1.running_mean\", \"face_decoder_blocks.2.0.conv_block.1.running_var\", \"face_decoder_blocks.2.1.conv_block.0.weight\", \"face_decoder_blocks.2.1.conv_block.0.bias\", \"face_decoder_blocks.2.1.conv_block.1.weight\", \"face_decoder_blocks.2.1.conv_block.1.bias\", \"face_decoder_blocks.2.1.conv_block.1.running_mean\", \"face_decoder_blocks.2.1.conv_block.1.running_var\", \"face_decoder_blocks.2.2.conv_block.0.weight\", \"face_decoder_blocks.2.2.conv_block.0.bias\", \"face_decoder_blocks.2.2.conv_block.1.weight\", \"face_decoder_blocks.2.2.conv_block.1.bias\", \"face_decoder_blocks.2.2.conv_block.1.running_mean\", \"face_decoder_blocks.2.2.conv_block.1.running_var\", \"face_decoder_blocks.3.0.conv_block.0.weight\", \"face_decoder_blocks.3.0.conv_block.0.bias\", \"face_decoder_blocks.3.0.conv_block.1.weight\", \"face_decoder_blocks.3.0.conv_block.1.bias\", \"face_decoder_blocks.3.0.conv_block.1.running_mean\", \"face_decoder_blocks.3.0.conv_block.1.running_var\", \"face_decoder_blocks.3.1.conv_block.0.weight\", \"face_decoder_blocks.3.1.conv_block.0.bias\", \"face_decoder_blocks.3.1.conv_block.1.weight\", \"face_decoder_blocks.3.1.conv_block.1.bias\", \"face_decoder_blocks.3.1.conv_block.1.running_mean\", \"face_decoder_blocks.3.1.conv_block.1.running_var\", \"face_decoder_blocks.3.2.conv_block.0.weight\", \"face_decoder_blocks.3.2.conv_block.0.bias\", \"face_decoder_blocks.3.2.conv_block.1.weight\", \"face_decoder_blocks.3.2.conv_block.1.bias\", \"face_decoder_blocks.3.2.conv_block.1.running_mean\", \"face_decoder_blocks.3.2.conv_block.1.running_var\", \"face_decoder_blocks.4.0.conv_block.0.weight\", \"face_decoder_blocks.4.0.conv_block.0.bias\", \"face_decoder_blocks.4.0.conv_block.1.weight\", \"face_decoder_blocks.4.0.conv_block.1.bias\", \"face_decoder_blocks.4.0.conv_block.1.running_mean\", \"face_decoder_blocks.4.0.conv_block.1.running_var\", \"face_decoder_blocks.4.1.conv_block.0.weight\", \"face_decoder_blocks.4.1.conv_block.0.bias\", \"face_decoder_blocks.4.1.conv_block.1.weight\", \"face_decoder_blocks.4.1.conv_block.1.bias\", \"face_decoder_blocks.4.1.conv_block.1.running_mean\", \"face_decoder_blocks.4.1.conv_block.1.running_var\", \"face_decoder_blocks.4.2.conv_block.0.weight\", \"face_decoder_blocks.4.2.conv_block.0.bias\", \"face_decoder_blocks.4.2.conv_block.1.weight\", \"face_decoder_blocks.4.2.conv_block.1.bias\", \"face_decoder_blocks.4.2.conv_block.1.running_mean\", \"face_decoder_blocks.4.2.conv_block.1.running_var\", \"face_decoder_blocks.5.0.conv_block.0.weight\", \"face_decoder_blocks.5.0.conv_block.0.bias\", \"face_decoder_blocks.5.0.conv_block.1.weight\", \"face_decoder_blocks.5.0.conv_block.1.bias\", \"face_decoder_blocks.5.0.conv_block.1.running_mean\", \"face_decoder_blocks.5.0.conv_block.1.running_var\", \"face_decoder_blocks.5.1.conv_block.0.weight\", \"face_decoder_blocks.5.1.conv_block.0.bias\", \"face_decoder_blocks.5.1.conv_block.1.weight\", \"face_decoder_blocks.5.1.conv_block.1.bias\", \"face_decoder_blocks.5.1.conv_block.1.running_mean\", \"face_decoder_blocks.5.1.conv_block.1.running_var\", \"face_decoder_blocks.5.2.conv_block.0.weight\", \"face_decoder_blocks.5.2.conv_block.0.bias\", \"face_decoder_blocks.5.2.conv_block.1.weight\", \"face_decoder_blocks.5.2.conv_block.1.bias\", \"face_decoder_blocks.5.2.conv_block.1.running_mean\", \"face_decoder_blocks.5.2.conv_block.1.running_var\", \"face_decoder_blocks.6.0.conv_block.0.weight\", \"face_decoder_blocks.6.0.conv_block.0.bias\", \"face_decoder_blocks.6.0.conv_block.1.weight\", \"face_decoder_blocks.6.0.conv_block.1.bias\", \"face_decoder_blocks.6.0.conv_block.1.running_mean\", \"face_decoder_blocks.6.0.conv_block.1.running_var\", \"face_decoder_blocks.6.1.conv_block.0.weight\", \"face_decoder_blocks.6.1.conv_block.0.bias\", \"face_decoder_blocks.6.1.conv_block.1.weight\", \"face_decoder_blocks.6.1.conv_block.1.bias\", \"face_decoder_blocks.6.1.conv_block.1.running_mean\", \"face_decoder_blocks.6.1.conv_block.1.running_var\", \"face_decoder_blocks.6.2.conv_block.0.weight\", \"face_decoder_blocks.6.2.conv_block.0.bias\", \"face_decoder_blocks.6.2.conv_block.1.weight\", \"face_decoder_blocks.6.2.conv_block.1.bias\", \"face_decoder_blocks.6.2.conv_block.1.running_mean\", \"face_decoder_blocks.6.2.conv_block.1.running_var\", \"output_block.0.conv_block.0.weight\", \"output_block.0.conv_block.0.bias\", \"output_block.0.conv_block.1.weight\", \"output_block.0.conv_block.1.bias\", \"output_block.0.conv_block.1.running_mean\", \"output_block.0.conv_block.1.running_var\", \"output_block.1.weight\", \"output_block.1.bias\". \n\tUnexpected key(s) in state_dict: \"state_dict\", \"optimizer\", \"global_step\", \"global_epoch\". "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from models import Wav2Lip\n",
    "\n",
    "# Set the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create an instance of the Wav2Lip model\n",
    "model = Wav2Lip()\n",
    "\n",
    "# Load the pre-trained model state dictionary\n",
    "checkpoint_path = '//Users/vinaykumar/Documents/Documents/Vinay Data/Openin ass/checkpoints/wav2lip.pth'  # Replace with the correct path to the model checkpoint\n",
    "model_state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(model_state_dict)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load the video and audio\n",
    "video_path = ' /Users/vinaykumar/Documents/Documents/Vinay Data/Openin ass/input_vid.mp4'  # Replace with your input video file\n",
    "audio_path = '/Users/vinaykumar/Documents/Documents/Vinay Data/Openin ass/output10.wav'    # Replace with your input audio file\n",
    "\n",
    "# Read video frames and audio\n",
    "video_reader = cv2.VideoCapture(video_path)\n",
    "fps = int(video_reader.get(cv2.CAP_PROP_FPS))\n",
    "audio, sr = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "# Initialize variables\n",
    "frame_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "frames = []\n",
    "frame_height, frame_width = None, None\n",
    "\n",
    "for i in range(frame_count):\n",
    "    success, frame = video_reader.read()\n",
    "    if not success:\n",
    "        break\n",
    "    if frame_height is None or frame_width is None:\n",
    "        frame_height, frame_width, _ = frame.shape\n",
    "\n",
    "    frames.append(frame)\n",
    "\n",
    "# Process frames and audio to create a lip-synced video\n",
    "results = []\n",
    "for i, frame in enumerate(frames):\n",
    "    if i % fps == 0:\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_batch = torch.from_numpy(frame_rgb).unsqueeze(0).to(device)\n",
    "\n",
    "        # Ensure audio has the same length as the frames\n",
    "        audio_batch = audio[i * 16000 // fps:(i + 1) * 16000 // fps]\n",
    "\n",
    "        # Generate lip-synced frame\n",
    "        result = model.inference(frame_batch, audio_batch)\n",
    "        result = result.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "        # Convert back to BGR format\n",
    "        result = cv2.cvtColor(result, cv2.COLOR_RGB2BGR)\n",
    "        results.append(result)\n",
    "\n",
    "# Save the output video\n",
    "output_path = '/Users/vinaykumar/Documents/Documents/Vinay Data/Openin ass/output_video.mp4'  # Replace with your desired output path\n",
    "output_video = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "for result in results:\n",
    "    output_video.write(result)\n",
    "\n",
    "output_video.release()\n",
    "video_reader.release()\n",
    "\n",
    "print(\"Lip-synced video saved at:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
